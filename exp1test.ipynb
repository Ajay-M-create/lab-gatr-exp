{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# setup_and_imports.ipynb\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch_geometric as pyg\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from lab_gatr import PointCloudPoolingScales, LaBGATr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_loading.ipynb\n",
    "import os\n",
    "from scipy.spatial import ConvexHull\n",
    "\n",
    "class ConvexHullDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        self.file_names = sorted([f for f in os.listdir(data_dir) if f.endswith('.txt')])\n",
    "        self.samples = []\n",
    "        self._prepare_dataset()\n",
    "    \n",
    "    def _prepare_dataset(self):\n",
    "        for file_name in tqdm(self.file_names, desc=\"Loading data\"):\n",
    "            file_path = os.path.join(self.data_dir, file_name)\n",
    "            with open(file_path, 'r') as f:\n",
    "                lines = f.readlines()[1:]  # Skip header\n",
    "                points = []\n",
    "                for line in lines:\n",
    "                    x, y, z = map(float, line.strip().split())\n",
    "                    points.append([x, y, z])\n",
    "                points = np.array(points)\n",
    "                if points.shape[0] < 4:\n",
    "                    # Convex hull in 3D requires at least 4 non-coplanar points\n",
    "                    volume = 0.0\n",
    "                else:\n",
    "                    try:\n",
    "                        hull = ConvexHull(points)\n",
    "                        volume = hull.volume\n",
    "                    except:\n",
    "                        # In case points are coplanar or singular\n",
    "                        volume = 0.0\n",
    "                self.samples.append({'points': points, 'volume': volume})\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        points = sample['points']\n",
    "        volume = sample['volume']\n",
    "        return {'points': torch.tensor(points, dtype=torch.float32), 'volume': torch.tensor(volume, dtype=torch.float32)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 5000/5000 [00:00<00:00, 6895.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 4000\n",
      "Testing samples: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# dataloader_definition.ipynb\n",
    "def collate_fn(batch):\n",
    "    points = torch.stack([item['points'] for item in batch], dim=0)  # Shape: [batch_size, num_points, 3]\n",
    "    volumes = torch.stack([item['volume'] for item in batch], dim=0)  # Shape: [batch_size]\n",
    "    return {'points': points, 'volume': volumes}\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "data_dir = '3d_point_cloud_dataset'  # Ensure this path is correct\n",
    "dataset = ConvexHullDataset(data_dir)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Testing samples: {len(test_dataset)}\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom_data.py\n",
    "import torch\n",
    "\n",
    "class CustomData:\n",
    "    def __init__(self, points: torch.Tensor, volume: torch.Tensor, device: torch.device):\n",
    "        \"\"\"\n",
    "        Custom data class to include necessary attributes for LaBGATr.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        points : torch.Tensor\n",
    "            Tensor of shape [batch_size, num_points, 3]\n",
    "        volume : torch.Tensor\n",
    "            Tensor of shape [batch_size]\n",
    "        device : torch.device\n",
    "            Device to move tensors to\n",
    "        \"\"\"\n",
    "        self.points = points.to(device)        # [batch_size, num_points, 3]\n",
    "        self.volume = volume.to(device)        # [batch_size]\n",
    "        self.batch = torch.arange(self.points.size(0)).repeat_interleave(self.points.size(1)).to(device)  # [batch_size * num_points]\n",
    "        self.scale0_sampling_index = torch.zeros(self.points.size(0) * self.points.size(1), dtype=torch.long).to(device)  # [batch_size * num_points]\n",
    "        self.pos = self.points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from gatr.interface.point import embed_point, extract_point\n",
    "\n",
    "class GeometricAlgebraInterface:\n",
    "    num_input_channels = 1\n",
    "    num_output_channels = 1\n",
    "    num_input_scalars = 1\n",
    "    num_output_scalars = 1\n",
    "\n",
    "    @staticmethod\n",
    "    @torch.no_grad()\n",
    "    def embed(data):\n",
    "        \"\"\"\n",
    "        Embeds 3D points into multivectors.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : CustomData\n",
    "            An instance of CustomData containing 'points' and 'volume'.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        multivectors : torch.Tensor\n",
    "            Embedded multivectors of shape [batch_size * num_points, 1, 16]\n",
    "        scalars : torch.Tensor\n",
    "            Corresponding scalar features of shape [batch_size * num_points, 1]\n",
    "        \"\"\"\n",
    "        points = data.points  # [batch_size, num_points, 3]\n",
    "        volumes = data.volume  # [batch_size]\n",
    "\n",
    "        print(f\"[Embed] Points shape: {points.shape}\")      # Debug\n",
    "        print(f\"[Embed] Volumes shape: {volumes.shape}\")  # Debug\n",
    "\n",
    "        # Flatten points for embedding\n",
    "        batch_size, num_points, _ = points.shape\n",
    "        points_flat = points.view(-1, 3)  # [batch_size * num_points, 3]\n",
    "        print(f\"[Embed] Points_flat shape: {points_flat.shape}\")  # Debug\n",
    "\n",
    "        # Embed points into multivectors\n",
    "        multivectors = embed_point(points_flat)  # [batch_size * num_points, 16]\n",
    "        print(f\"[Embed] Multivectors shape after embedding: {multivectors.shape}\")  # Debug\n",
    "\n",
    "        # Reshape to [batch_size * num_points, 1, 16]\n",
    "        multivectors = multivectors.unsqueeze(1)  # [160, 1, 16]\n",
    "        print(f\"[Embed] Multivectors reshaped: {multivectors.shape}\")  # Debug\n",
    "\n",
    "        # Replicate volume for each point\n",
    "        scalars = volumes.view(batch_size, 1).repeat(1, num_points)  # [batch_size, num_points]\n",
    "        scalars = scalars.view(-1, 1)  # [160, 1]\n",
    "        print(f\"[Embed] Scalars shape: {scalars.shape}\")  # Debug\n",
    "\n",
    "        return multivectors, scalars\n",
    "\n",
    "    @staticmethod\n",
    "    def dislodge(multivectors, scalars):\n",
    "        \"\"\"\n",
    "        Extracts 3D points from multivectors.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        multivectors : torch.Tensor\n",
    "            Embedded multivectors of shape [batch_size * num_points, 1, 16]\n",
    "        scalars : torch.Tensor\n",
    "            Corresponding scalar features of shape [batch_size * num_points, 1]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        points : torch.Tensor\n",
    "            Extracted 3D points of shape [batch_size, num_points, 3]\n",
    "        \"\"\"\n",
    "        # Remove the singleton dimension\n",
    "        multivectors = multivectors.squeeze(1)  # [160, 16]\n",
    "        print(f\"[Dislodge] Multivectors shape after squeezing: {multivectors.shape}\")  # Debug\n",
    "\n",
    "        # Extract points from multivectors\n",
    "        points = extract_point(multivectors)  # [160, 3]\n",
    "        print(f\"[Dislodge] Points shape after extraction: {points.shape}\")  # Debug\n",
    "\n",
    "        # Reshape back to [batch_size, num_points, 3]\n",
    "        batch_size = scalars.shape[0] // 5  # Assuming num_points = 5\n",
    "        num_points = 5\n",
    "        points = points.view(batch_size, num_points, 3)\n",
    "        print(f\"[Dislodge] Points reshaped: {points.shape}\")  # Debug\n",
    "\n",
    "        return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer_model.ipynb\n",
    "class TransformerRegressor(nn.Module):\n",
    "    def __init__(self, input_dim=3, embed_dim=64, num_heads=8, num_layers=3, dropout=0.1):\n",
    "        super(TransformerRegressor, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, embed_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dropout=dropout)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.pooling = nn.AdaptiveAvgPool1d(1)\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [batch_size, num_points, 3]\n",
    "        \"\"\"\n",
    "        x = self.embedding(x)  # [batch_size, num_points, embed_dim]\n",
    "        x = x.permute(1, 0, 2)  # [num_points, batch_size, embed_dim] for Transformer\n",
    "        x = self.transformer(x)  # [num_points, batch_size, embed_dim]\n",
    "        x = x.permute(1, 2, 0)  # [batch_size, embed_dim, num_points]\n",
    "        x = self.pooling(x).squeeze(-1)  # [batch_size, embed_dim]\n",
    "        x = self.regressor(x).squeeze(-1)  # [batch_size]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LaB-GATr (261761 parameters)\n",
      "LaB-GATr (number of parameters): 261761\n"
     ]
    }
   ],
   "source": [
    "# gatr_model_initialization.ipynb\n",
    "#from geometric_algebra_interface import GeometricAlgebraInterface\n",
    "#from custom_data import CustomData  # Ensure correct import path\n",
    "\n",
    "# Initialize GATr model with the updated interface\n",
    "gatr_model = LaBGATr(\n",
    "    GeometricAlgebraInterface,\n",
    "    d_model=8,\n",
    "    num_blocks=10,\n",
    "    num_attn_heads=4,\n",
    "    use_class_token=False\n",
    ").to(device)\n",
    "\n",
    "print(f\"LaB-GATr (number of parameters): {sum(p.numel() for p in gatr_model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_model_transformer.ipynb\n",
    "def train_model_transformer(model, train_loader, test_loader, epochs=50, lr=1e-3):\n",
    "    \"\"\"\n",
    "    Training loop for the TransformerRegressor model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : torch.nn.Module\n",
    "        The TransformerRegressor model.\n",
    "    train_loader : torch.utils.data.DataLoader\n",
    "        DataLoader for the training set.\n",
    "    test_loader : torch.utils.data.DataLoader\n",
    "        DataLoader for the testing set.\n",
    "    epochs : int\n",
    "        Number of training epochs.\n",
    "    lr : float\n",
    "        Learning rate.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        Training and testing losses.\n",
    "    \"\"\"\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            # Extract 'points' and 'volume' from the batch\n",
    "            points = batch['points'].to(device)    # [batch_size, num_points, 3]\n",
    "            volumes = batch['volume'].to(device)  # [batch_size]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(points)                # [batch_size]\n",
    "            loss = criterion(outputs, volumes)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * points.size(0)\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        train_losses.append(epoch_loss)\n",
    "        \n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                points = batch['points'].to(device)\n",
    "                volumes = batch['volume'].to(device)\n",
    "                outputs = model(points)\n",
    "                loss = criterion(outputs, volumes)\n",
    "                test_loss += loss.item() * points.size(0)\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch}/{epochs} - Train Loss: {epoch_loss:.4f}, Test Loss: {test_loss:.4f}\")\n",
    "    \n",
    "    return train_losses, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_model_gatr.ipynb\n",
    "#from custom_data import CustomData  # Ensure correct import path\n",
    "\n",
    "def train_model_gatr(model, train_loader, test_loader, epochs=50, lr=1e-3):\n",
    "    \"\"\"\n",
    "    Training loop for the GATr model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : torch.nn.Module\n",
    "        The GATr model.\n",
    "    train_loader : torch.utils.data.DataLoader\n",
    "        DataLoader for the training set.\n",
    "    test_loader : torch.utils.data.DataLoader\n",
    "        DataLoader for the testing set.\n",
    "    epochs : int\n",
    "        Number of training epochs.\n",
    "    lr : float\n",
    "        Learning rate.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        Training and testing losses.\n",
    "    \"\"\"\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            # Instantiate CustomData\n",
    "            custom_data = CustomData(batch['points'], batch['volume'], device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(custom_data)  # Pass the CustomData object\n",
    "            loss = criterion(outputs, custom_data.volume)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * custom_data.points.size(0)\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        train_losses.append(epoch_loss)\n",
    "        \n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                custom_data = CustomData(batch['points'], batch['volume'], device)\n",
    "                outputs = model(custom_data)\n",
    "                loss = criterion(outputs, custom_data.volume)\n",
    "                test_loss += loss.item() * custom_data.points.size(0)\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch}/{epochs} - Train Loss: {epoch_loss:.4f}, Test Loss: {test_loss:.4f}\")\n",
    "    \n",
    "    return train_losses, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 'points' shape: torch.Size([32, 5, 3])\n",
      "Batch 'volume' shape: torch.Size([32])\n",
      "CustomData.batch shape: torch.Size([160])\n",
      "CustomData.scale0_sampling_index shape: torch.Size([160])\n",
      "[Embed] Points shape: torch.Size([32, 5, 3])\n",
      "[Embed] Volumes shape: torch.Size([32])\n",
      "[Embed] Points_flat shape: torch.Size([160, 3])\n",
      "[Embed] Multivectors shape after embedding: torch.Size([160, 16])\n",
      "[Embed] Multivectors reshaped: torch.Size([160, 1, 16])\n",
      "[Embed] Scalars shape: torch.Size([160, 1])\n",
      "Error during forward pass: \n"
     ]
    }
   ],
   "source": [
    "# test_forward_pass.ipynb\n",
    "#from custom_data import CustomData  # Ensure correct import path\n",
    "\n",
    "# Fetch a single batch from the train_loader\n",
    "batch = next(iter(train_loader))\n",
    "print(f\"Batch 'points' shape: {batch['points'].shape}\")   # Expected: [32, 5, 3]\n",
    "print(f\"Batch 'volume' shape: {batch['volume'].shape}\") # Expected: [32]\n",
    "\n",
    "# Create CustomData instance\n",
    "custom_data = CustomData(batch['points'], batch['volume'], device)\n",
    "print(f\"CustomData.batch shape: {custom_data.batch.shape}\")  # Expected: [160]\n",
    "print(f\"CustomData.scale0_sampling_index shape: {custom_data.scale0_sampling_index.shape}\")  # Expected: [160]\n",
    "\n",
    "# Pass through the model\n",
    "try:\n",
    "    outputs = gatr_model(custom_data)\n",
    "    print(f\"Model outputs shape: {outputs.shape}\")           # Expected: [32]\n",
    "except Exception as e:\n",
    "    print(f\"Error during forward pass: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ajay/anaconda3/envs/anatomygen_gatr/lib/python3.10/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Train Loss: 879.0779, Test Loss: 623.7451\n",
      "Epoch 2/50 - Train Loss: 583.0120, Test Loss: 616.7980\n",
      "Epoch 3/50 - Train Loss: 574.8533, Test Loss: 583.0727\n",
      "Epoch 4/50 - Train Loss: 539.6826, Test Loss: 608.2105\n",
      "Epoch 5/50 - Train Loss: 587.2325, Test Loss: 617.1289\n",
      "Epoch 6/50 - Train Loss: 588.2393, Test Loss: 616.6116\n",
      "Epoch 7/50 - Train Loss: 587.8776, Test Loss: 617.1104\n",
      "Epoch 8/50 - Train Loss: 586.4686, Test Loss: 617.0470\n",
      "Epoch 9/50 - Train Loss: 569.8059, Test Loss: 551.1265\n",
      "Epoch 10/50 - Train Loss: 500.9128, Test Loss: 474.3313\n",
      "Epoch 11/50 - Train Loss: 478.6215, Test Loss: 505.7599\n",
      "Epoch 12/50 - Train Loss: 464.4069, Test Loss: 454.4986\n",
      "Epoch 13/50 - Train Loss: 439.7087, Test Loss: 449.5160\n",
      "Epoch 14/50 - Train Loss: 432.3169, Test Loss: 428.3028\n",
      "Epoch 15/50 - Train Loss: 400.9624, Test Loss: 423.5585\n",
      "Epoch 16/50 - Train Loss: 382.0725, Test Loss: 352.9266\n",
      "Epoch 17/50 - Train Loss: 349.6240, Test Loss: 346.4014\n",
      "Epoch 18/50 - Train Loss: 337.4675, Test Loss: 315.7315\n",
      "Epoch 19/50 - Train Loss: 306.7215, Test Loss: 242.2402\n",
      "Epoch 20/50 - Train Loss: 279.7410, Test Loss: 211.1309\n",
      "Epoch 21/50 - Train Loss: 244.2350, Test Loss: 262.6864\n",
      "Epoch 22/50 - Train Loss: 233.2761, Test Loss: 197.3710\n",
      "Epoch 23/50 - Train Loss: 231.8749, Test Loss: 184.7643\n",
      "Epoch 24/50 - Train Loss: 221.0584, Test Loss: 168.9650\n",
      "Epoch 25/50 - Train Loss: 196.6792, Test Loss: 169.3524\n",
      "Epoch 26/50 - Train Loss: 177.0996, Test Loss: 157.8669\n",
      "Epoch 27/50 - Train Loss: 172.5103, Test Loss: 323.9579\n",
      "Epoch 28/50 - Train Loss: 158.1414, Test Loss: 174.5381\n",
      "Epoch 29/50 - Train Loss: 152.2246, Test Loss: 132.6400\n",
      "Epoch 30/50 - Train Loss: 175.0337, Test Loss: 120.3925\n",
      "Epoch 31/50 - Train Loss: 162.5172, Test Loss: 119.3967\n",
      "Epoch 32/50 - Train Loss: 144.1270, Test Loss: 141.2393\n",
      "Epoch 33/50 - Train Loss: 143.1675, Test Loss: 112.7985\n",
      "Epoch 34/50 - Train Loss: 127.4823, Test Loss: 85.8237\n",
      "Epoch 35/50 - Train Loss: 126.4346, Test Loss: 96.1167\n",
      "Epoch 36/50 - Train Loss: 111.5985, Test Loss: 95.9136\n",
      "Epoch 37/50 - Train Loss: 111.3401, Test Loss: 99.2376\n",
      "Epoch 38/50 - Train Loss: 117.3590, Test Loss: 109.6622\n",
      "Epoch 39/50 - Train Loss: 104.3668, Test Loss: 92.4086\n",
      "Epoch 40/50 - Train Loss: 108.2813, Test Loss: 124.2231\n",
      "Epoch 41/50 - Train Loss: 104.8265, Test Loss: 99.5608\n",
      "Epoch 42/50 - Train Loss: 116.2909, Test Loss: 69.0500\n",
      "Epoch 43/50 - Train Loss: 93.1418, Test Loss: 80.8388\n",
      "Epoch 44/50 - Train Loss: 100.4455, Test Loss: 66.4028\n",
      "Epoch 45/50 - Train Loss: 85.6493, Test Loss: 81.5041\n",
      "Epoch 46/50 - Train Loss: 96.4564, Test Loss: 59.4579\n",
      "Epoch 47/50 - Train Loss: 89.1538, Test Loss: 131.0992\n",
      "Epoch 48/50 - Train Loss: 84.0753, Test Loss: 56.1463\n",
      "Epoch 49/50 - Train Loss: 92.2221, Test Loss: 58.9699\n",
      "Epoch 50/50 - Train Loss: 83.8769, Test Loss: 83.1157\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# train_transformer.ipynb\n",
    "# Initialize TransformerRegressor model\n",
    "transformer_model = TransformerRegressor().to(device)\n",
    "\n",
    "# Train the Transformer model\n",
    "transformer_train_losses, transformer_test_losses = train_model_transformer(\n",
    "    transformer_model,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    epochs=50,\n",
    "    lr=1e-3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LaB-GATr (261761 parameters)\n",
      "[Embed] Points shape: torch.Size([32, 5, 3])\n",
      "[Embed] Volumes shape: torch.Size([32])\n",
      "[Embed] Points_flat shape: torch.Size([160, 3])\n",
      "[Embed] Multivectors shape after embedding: torch.Size([160, 16])\n",
      "[Embed] Multivectors reshaped: torch.Size([160, 1, 16])\n",
      "[Embed] Scalars shape: torch.Size([160, 1])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 15\u001b[0m\n\u001b[1;32m      6\u001b[0m gatr_model \u001b[38;5;241m=\u001b[39m LaBGATr(\n\u001b[1;32m      7\u001b[0m     GeometricAlgebraInterface,\n\u001b[1;32m      8\u001b[0m     d_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     use_class_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     12\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Verify the model with a single forward pass (if not done in previous cell)\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mgatr_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcustom_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel outputs shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutputs\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Expected: [32]\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Train the GATr model\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/anatomygen_gatr/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/anatomygen_gatr/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/lab-gatr/lab_gatr/models/lab_gatr.py:71\u001b[0m, in \u001b[0;36mLaBGATr.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, data: Data) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m---> 71\u001b[0m     multivectors, scalars, reference_multivector \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokeniser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m     multivectors, scalars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgatr(\n\u001b[1;32m     74\u001b[0m         multivectors,\n\u001b[1;32m     75\u001b[0m         scalars\u001b[38;5;241m=\u001b[39mscalars,\n\u001b[1;32m     76\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mget_attn_mask(data\u001b[38;5;241m.\u001b[39mbatch[data\u001b[38;5;241m.\u001b[39mscale0_sampling_index] \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39mbatch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m data\u001b[38;5;241m.\u001b[39mbatch),\n\u001b[1;32m     77\u001b[0m         join_reference\u001b[38;5;241m=\u001b[39mreference_multivector\n\u001b[1;32m     78\u001b[0m     )\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokeniser\u001b[38;5;241m.\u001b[39mlift(multivectors, scalars)\n",
      "File \u001b[0;32m~/anaconda3/envs/anatomygen_gatr/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/anatomygen_gatr/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/lab-gatr/lab_gatr/models/lab_gatr.py:327\u001b[0m, in \u001b[0;36mCrossAttentionTokeniser.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    317\u001b[0m multivectors, scalars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeometric_algebra_interface\u001b[38;5;241m.\u001b[39membed(data)\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmultivectors\u001b[39m\u001b[38;5;124m'\u001b[39m: multivectors,\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscalars\u001b[39m\u001b[38;5;124m'\u001b[39m: scalars,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpos\u001b[39m\u001b[38;5;124m'\u001b[39m: data\u001b[38;5;241m.\u001b[39mpos[data\u001b[38;5;241m.\u001b[39mscale0_sampling_index]\n\u001b[1;32m    325\u001b[0m }\n\u001b[0;32m--> 327\u001b[0m attn_mask \u001b[38;5;241m=\u001b[39m \u001b[43mget_attn_mask\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale0_sampling_index\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[43msource_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m reference_multivector \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreference_multivector\u001b[39m\u001b[38;5;124m'\u001b[39m][data\u001b[38;5;241m.\u001b[39mscale0_sampling_index]\n\u001b[1;32m    334\u001b[0m multivectors, scalars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_attention_hatchling(\n\u001b[1;32m    335\u001b[0m     multivectors_source\u001b[38;5;241m=\u001b[39mmultivectors,\n\u001b[1;32m    336\u001b[0m     multivectors_target\u001b[38;5;241m=\u001b[39mmultivectors[data\u001b[38;5;241m.\u001b[39mscale0_sampling_index],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    340\u001b[0m     reference_multivector\u001b[38;5;241m=\u001b[39mreference_multivector\n\u001b[1;32m    341\u001b[0m )\n",
      "File \u001b[0;32m~/lab-gatr/lab_gatr/nn/attn_mask.py:11\u001b[0m, in \u001b[0;36mget_attn_mask\u001b[0;34m(target_batch, source_batch)\u001b[0m\n\u001b[1;32m      8\u001b[0m     attn_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 11\u001b[0m     attn_mask \u001b[38;5;241m=\u001b[39m \u001b[43mBlockDiagonalMask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_seqlens\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mq_seqlen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbincount\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkv_seqlen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbincount\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msource_batch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attn_mask\n",
      "File \u001b[0;32m~/anaconda3/envs/anatomygen_gatr/lib/python3.10/site-packages/xformers/ops/fmha/attn_bias.py:725\u001b[0m, in \u001b[0;36mBlockDiagonalMask.from_seqlens\u001b[0;34m(cls, q_seqlen, kv_seqlen, device)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates a :attr:`BlockDiagonalMask` from a list of tensors lengths for query and key/value.\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \n\u001b[1;32m    717\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;124;03m    BlockDiagonalMask\u001b[39;00m\n\u001b[1;32m    723\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    724\u001b[0m device \u001b[38;5;241m=\u001b[39m _get_default_bias_device(device)\n\u001b[0;32m--> 725\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m kv_seqlen \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(q_seqlen) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(kv_seqlen)\n\u001b[1;32m    726\u001b[0m q_seqinfo \u001b[38;5;241m=\u001b[39m _SeqLenInfo\u001b[38;5;241m.\u001b[39mfrom_seqlens(q_seqlen, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    727\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kv_seqlen \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m q_seqlen \u001b[38;5;241m==\u001b[39m kv_seqlen:\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train_gatr.ipynb\n",
    "#from geometric_algebra_interface import GeometricAlgebraInterface  # Update the path accordingly\n",
    "#from custom_data import CustomData  # Ensure correct import path\n",
    "\n",
    "# Initialize GATr model with the updated interface\n",
    "gatr_model = LaBGATr(\n",
    "    GeometricAlgebraInterface,\n",
    "    d_model=8,\n",
    "    num_blocks=10,\n",
    "    num_attn_heads=4,\n",
    "    use_class_token=False\n",
    ").to(device)\n",
    "\n",
    "# Verify the model with a single forward pass (if not done in previous cell)\n",
    "outputs = gatr_model(custom_data)\n",
    "print(f\"Model outputs shape: {outputs.shape}\")  # Expected: [32]\n",
    "\n",
    "# Train the GATr model\n",
    "gatr_train_losses, gatr_test_losses = train_model_gatr(\n",
    "    gatr_model,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    epochs=50,\n",
    "    lr=1e-3\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anatomygen_gatr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
